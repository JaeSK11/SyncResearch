# SyncResearch

A self-hosted RAG (Retrieval-Augmented Generation) system for analyzing and querying academic research papers locally with complete data privacy.

## âœ¨ Features

- ğŸ“„ **PDF Analysis** - Upload and query research papers using natural language
- ğŸ” **Semantic Search** - Vector-based similarity search across documents
- ğŸ¤– **Local LLM** - Self-hosted Llama 3.1 8B via vLLM (no API costs)
- ğŸ”’ **Complete Privacy** - All data and processing stays on your hardware
- ğŸ’¾ **Persistent Storage** - MinIO (S3-compatible) + ChromaDB vector database
- ğŸŒ **Modern UI** - Clean React interface with real-time health monitoring

## ğŸš€ Quick Start

### Prerequisites

- NVIDIA GPU (tested on RTX 3090, 24GB VRAM)
- Docker with NVIDIA Container Runtime
- Node.js 20+
- Python 3.10+

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/SyncResearch.git
cd SyncResearch
```

2. **Set up Python environment**
```bash
python -m venv rrenv
source rrenv/bin/activate
pip install -r requirements.txt
```

3. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your MinIO credentials
```

4. **Install frontend dependencies**
```bash
cd frontend
npm install
cd ..
```

### Running the System

**Start all services:**

```bash
# 1. Start vLLM server (loads Llama 3.1 8B)
cd vllm-server
make llama31

# 2. Start FastAPI backend (in new terminal)
cd ..
source rrenv/bin/activate
uvicorn api.main:app --reload --host 0.0.0.0 --port 8080

# 3. Start React frontend (in new terminal)
cd frontend
npm run dev
```

**Access the application:**
- Frontend: http://localhost:5173
- API Docs: http://localhost:8080/docs
- vLLM: http://localhost:8000

## ğŸ“– Usage

1. **Upload a research paper** - Click "Upload PDF" and select a paper
2. **Wait for processing** - The system extracts structure and creates embeddings
3. **Ask questions** - Type queries like:
   - "What is the main contribution of this paper?"
   - "Explain the methodology used"
   - "What are the key results?"
4. **View sources** - See which paper sections informed the answer

## ğŸ› ï¸ Tech Stack

**Backend:**
- [vLLM](https://github.com/vllm-project/vllm) - High-performance LLM inference
- [FastAPI](https://fastapi.tiangolo.com/) - REST API framework
- [ChromaDB](https://www.trychroma.com/) - Vector database
- [Docling](https://github.com/DS4SD/docling) - PDF structure extraction
- [MinIO](https://min.io/) - S3-compatible object storage

**Frontend:**
- [React](https://react.dev/) - UI framework
- [Vite](https://vitejs.dev/) - Build tool
- [Lucide](https://lucide.dev/) - Icons

**Models:**
- Llama 3.1 8B Instruct (LLM)
- BAAI/bge-base-en-v1.5 (Embeddings)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  React Frontend â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI Server â”‚â—„â”€â”€â”€â”€â–ºâ”‚ MinIO Storageâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ChromaDBâ”‚ â”‚vLLM     â”‚
â”‚Vectors â”‚ â”‚Llama 3.1â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
SyncResearch/
â”œâ”€â”€ api/                  # FastAPI backend
â”‚   â”œâ”€â”€ routes/          # API endpoints
â”‚   â””â”€â”€ models/          # Pydantic schemas
â”œâ”€â”€ frontend/            # React application
â”œâ”€â”€ document_processing/ # PDF parsing & chunking
â”œâ”€â”€ embeddings_module/   # Vector generation
â”œâ”€â”€ vectordb/           # ChromaDB operations
â”œâ”€â”€ storage/            # MinIO client
â”œâ”€â”€ rag_pipeline/       # RAG implementation
â””â”€â”€ vllm-server/        # LLM inference setup
```

## ğŸ”® Roadmap

### âœ… Current (v1.0)
- [x] Single-paper RAG queries
- [x] PDF upload and processing
- [x] Local LLM inference
- [x] Web interface
- [x] Source attribution

### ğŸš§ Phase 2 (Multi-Agent)
- [ ] Query decomposition with Granite 3.0
- [ ] Parallel paper analysis
- [ ] Multi-paper comparison
- [ ] Advanced comparison synthesis

### ğŸ“‹ Phase 3 (Advanced)
- [ ] Citation network analysis
- [ ] GPU-accelerated embeddings
- [ ] Graph RAG implementation
- [ ] Figure/diagram processing

## âš™ï¸ Configuration

Key environment variables (`.env`):

```bash
# MinIO Storage
MINIO_ENDPOINT=localhost:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
MINIO_BUCKET=research-papers

# vLLM
VLLM_URL=http://localhost:8000

# App
DEBUG_MODE=False
```

## ğŸ§ª Testing

```bash
# Run tests
pytest tests/

# Test specific component
pytest tests/test_rag.py -v
```

## ğŸ“Š Performance

- **Query Latency:** ~2.5s end-to-end
- **LLM Speed:** 25-30 tokens/second
- **Embedding:** ~45 chunks/second (CPU)
- **Concurrent Users:** 1-5 (single GPU)

## ğŸ¤ Contributing

Contributions welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details

## ğŸ™ Acknowledgments

- Built with [vLLM](https://github.com/vllm-project/vllm) for efficient LLM inference
- PDF processing powered by [Docling](https://github.com/DS4SD/docling)
- Inspired by research in retrieval-augmented generation

## ğŸ“® Contact

Questions? Open an issue or reach out:
- GitHub Issues: [Project Issues](https://github.com/yourusername/SyncResearch/issues)
- Email: your.email@example.com

---

**Note:** Requires NVIDIA GPU. For CPU-only inference, see the [CPU deployment guide](docs/cpu-deployment.md).